{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cfa446",
   "metadata": {},
   "source": [
    "# <center>ML Final Project</center>\n",
    "\n",
    "## <center>Bagging & Boosting</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c206c4",
   "metadata": {},
   "source": [
    "### <center>Bagging - </center>\n",
    "\n",
    "#### \"Bootstrap Aggregation,\" or \"bagging,\" is a method used to lower a predictive model's variance. It is a parallel ensemble method that fits each model independently of the others while training many models at once. By producing new data from the existing dataset, bagging improves the training process. By drawing samples with replacement, a technique known as bootstrap sampling enables certain observations to be reproduced across many training sets. In the freshly created datasets, every element in the original dataset has an equal chance of being chosen.\n",
    "\n",
    "\n",
    "### <center>Boosting - </center>\n",
    "\n",
    "#### Boosting is a sequential ensemble technique where the weights of the observations are iteratively adjusted according to the most recent classification performance. An observation's weight is increased upon wrong classification, meaning that in succeeding cycles, more difficult examples will be the focus. Boosting essentially attempts to increase the prediction capacity and decrease bias error of the model by making weak learners stronger. Misclassified data points have their weights increased during each iteration, increasing their influence in the subsequent training cycle. During training, the boosting approach gives each model a weight, increasing the weight of the models that do well. As boosting advances, it records mistakes made by earlier models to inform the instruction of current learners.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Research paper URL - \"https://arxiv.org/pdf/1904.08067v5.pdf\"\n",
    "\n",
    "#### Dataset URL - \"https://github.com/kk7nc/Text_Classification/tree/master/Data\"\n",
    "\n",
    "#### Github URL - \"https://github.com/kk7nc/Text_Classification/tree/master/code\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1e9c3",
   "metadata": {},
   "source": [
    "### Dataset loaded..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1246455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    " * Last Update: 04/25/2018\n",
    " * This file is part of  RMDL project, University of Virginia.\n",
    " * Free to use, change, share and distribute source code of RMDL\n",
    " * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n",
    " * Comments and Error: email: kk7nc@virginia.edu\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, tarfile\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "# image shape\n",
    "\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = '.\\Glove'\n",
    "\n",
    "# url of the binary data\n",
    "\n",
    "\n",
    "\n",
    "# path to the binary train file with image data\n",
    "\n",
    "\n",
    "def download_and_extract(data='Wikipedia'):\n",
    "    \"\"\"\n",
    "    Download and extract the GloVe\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if data=='Wikipedia':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    elif data=='Common_Crawl_840B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
    "    elif data=='Common_Crawl_42B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
    "    elif data=='Twitter':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
    "    else:\n",
    "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    print(filepath)\n",
    "\n",
    "    path = os.path.abspath(dest_directory)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
    "\n",
    "\n",
    "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        zip_ref.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdfe57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    " * Last Update: 04/25/2018\n",
    " * This file is part of  RMDL project, University of Virginia.\n",
    " * Free to use, change, share and distribute source code of RMDL\n",
    " * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n",
    " * Comments and Error: email: kk7nc@virginia.edu\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, tarfile\n",
    "import numpy as np\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "# image shape\n",
    "\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = '.\\data_WOS'\n",
    "\n",
    "# url of the binary data\n",
    "DATA_URL = 'http://kowsari.net/WebOfScience.tar.gz'\n",
    "\n",
    "\n",
    "# path to the binary train file with image data\n",
    "\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the WOS datasets\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "\n",
    "\n",
    "    path = os.path.abspath(dest_directory)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "\n",
    "        print('Downloaded', filename)\n",
    "\n",
    "        tarfile.open(filepath, 'r').extractall(dest_directory)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9230e",
   "metadata": {},
   "source": [
    "# Bagging : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f416a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', BaggingClassifier(KNeighborsClassifier())),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58680bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65       319\n",
      "           1       0.59      0.59      0.59       389\n",
      "           2       0.63      0.56      0.59       394\n",
      "           3       0.57      0.57      0.57       392\n",
      "           4       0.60      0.56      0.58       385\n",
      "           5       0.69      0.61      0.65       395\n",
      "           6       0.61      0.45      0.52       390\n",
      "           7       0.74      0.72      0.73       396\n",
      "           8       0.81      0.83      0.82       398\n",
      "           9       0.74      0.74      0.74       397\n",
      "          10       0.81      0.85      0.83       399\n",
      "          11       0.75      0.83      0.79       396\n",
      "          12       0.70      0.49      0.58       393\n",
      "          13       0.81      0.54      0.65       396\n",
      "          14       0.75      0.78      0.76       394\n",
      "          15       0.71      0.78      0.74       398\n",
      "          16       0.72      0.73      0.72       364\n",
      "          17       0.62      0.79      0.70       376\n",
      "          18       0.44      0.67      0.53       310\n",
      "          19       0.52      0.54      0.53       251\n",
      "\n",
      "    accuracy                           0.67      7532\n",
      "   macro avg       0.67      0.67      0.66      7532\n",
      "weighted avg       0.68      0.67      0.67      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3c925",
   "metadata": {},
   "source": [
    "# Boosting :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c8c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.2026           65.98m\n",
      "         2           1.9756           70.43m\n",
      "         3           1.8175           68.44m\n",
      "         4           1.6981           64.51m\n",
      "         5           1.6015           61.62m\n",
      "         6           1.5210           60.36m\n",
      "         7           1.4468           59.33m\n",
      "         8           1.3821           58.40m\n",
      "         9           1.3284           56.43m\n",
      "        10           1.2772           54.28m\n",
      "        11           1.2323           52.81m\n",
      "        12           1.1904           51.35m\n",
      "        13           1.1532           49.94m\n",
      "        14           1.1182           48.93m\n",
      "        15           1.0865           47.98m\n",
      "        16           1.0557           46.47m\n",
      "        17           1.0273           45.20m\n",
      "        18           0.9991           43.77m\n",
      "        19           0.9739           42.70m\n",
      "        20           0.9495           41.63m\n",
      "        21           0.9245           40.35m\n",
      "        22           0.9053           39.26m\n",
      "        23           0.8825           37.76m\n",
      "        24           0.8644           36.07m\n",
      "        25           0.8432           34.49m\n",
      "        26           0.8254           32.95m\n",
      "        27           0.8061           31.64m\n",
      "        28           0.7900           30.17m\n",
      "        29           0.7753           28.92m\n",
      "        30           0.7607           27.63m\n",
      "        31           0.7476           26.19m\n",
      "        32           0.7352           24.74m\n",
      "        33           0.7224           23.31m\n",
      "        34           0.7078           22.01m\n",
      "        35           0.6962           20.63m\n",
      "        36           0.6851           19.24m\n",
      "        37           0.6734           17.83m\n",
      "        38           0.6628           16.44m\n",
      "        39           0.6526           15.03m\n",
      "        40           0.6412           13.66m\n",
      "        41           0.6324           12.27m\n",
      "        42           0.6229           10.88m\n",
      "        43           0.6139            9.51m\n",
      "        44           0.6052            8.15m\n",
      "        45           0.5969            6.77m\n",
      "        46           0.5884            5.43m\n",
      "        47           0.5808            4.09m\n",
      "        48           0.5721            2.73m\n",
      "        49           0.5638            1.37m\n",
      "        50           0.5558            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier(n_estimators=50,verbose=2)),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4a3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       319\n",
      "           1       0.68      0.67      0.67       389\n",
      "           2       0.70      0.68      0.69       394\n",
      "           3       0.65      0.72      0.68       392\n",
      "           4       0.78      0.78      0.78       385\n",
      "           5       0.84      0.61      0.70       395\n",
      "           6       0.82      0.84      0.83       390\n",
      "           7       0.86      0.72      0.78       396\n",
      "           8       0.89      0.85      0.87       398\n",
      "           9       0.92      0.85      0.88       397\n",
      "          10       0.93      0.86      0.90       399\n",
      "          11       0.89      0.81      0.85       396\n",
      "          12       0.29      0.69      0.41       393\n",
      "          13       0.87      0.68      0.76       396\n",
      "          14       0.85      0.81      0.83       394\n",
      "          15       0.84      0.86      0.85       398\n",
      "          16       0.65      0.79      0.71       364\n",
      "          17       0.96      0.73      0.83       376\n",
      "          18       0.75      0.53      0.62       310\n",
      "          19       0.64      0.54      0.58       251\n",
      "\n",
      "    accuracy                           0.74      7532\n",
      "   macro avg       0.78      0.73      0.75      7532\n",
      "weighted avg       0.78      0.74      0.75      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bc42f",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### The boosting algorithm fared better than the bagging technique according to the performance measures. The boosting algorithm's f1-score was 0.75, whereas the bagging algorithm's was 0.67. This suggests that although both methods yielded respectable results on the data, boosting was more successful. For this specific job, boosting was a better option because of the higher f1-score, which indicates that boosting was more successful in striking a balance between precision and recall. In summary, boosting outperformed bagging in this comparison, demonstrating its benefit in improving model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020ae71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
